{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "回顾多层感知机"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-07-10T12:58:07.898460Z",
     "end_time": "2023-07-10T12:58:07.940345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.1620, -0.0793,  0.1431,  0.0673,  0.0966,  0.1359, -0.1023, -0.0286,\n          0.0487, -0.1550],\n        [-0.1697, -0.0233,  0.1816,  0.0134,  0.0471,  0.0944, -0.2029,  0.0183,\n          0.1670, -0.1843]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20,256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20) # 2 * 20\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "自定义块"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T13:50:10.192072Z",
     "end_time": "2023-07-10T13:50:10.206034Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0184,  0.1097,  0.0730,  0.0487, -0.0836,  0.2052, -0.0275,  0.3346,\n          0.1413,  0.1403],\n        [-0.0284,  0.0834,  0.1159,  0.0303, -0.1578,  0.1395,  0.0090,  0.2734,\n          0.1115,  0.0310]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T14:08:08.690058Z",
     "end_time": "2023-07-10T14:08:08.728952Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 3.0204e-01, -3.7004e-01, -4.7854e-01, -1.4433e-02, -3.4079e-01,\n          2.3749e-01, -1.7188e-02,  2.8027e-01,  1.3271e-01,  1.8961e-01,\n          2.3601e-01, -5.6924e-01,  6.1351e-01, -1.6358e-01,  2.7222e-01,\n          6.1518e-04,  1.7524e-01, -4.7493e-01,  1.3230e-01, -1.9324e-01,\n          3.4212e-01, -1.7250e-01, -7.5536e-02, -1.5026e-01, -4.0451e-01,\n          2.0170e-01, -1.1454e-01, -1.6821e-01,  4.1968e-01,  2.0275e-01,\n          3.2770e-02,  5.5130e-02, -7.7672e-01, -2.5013e-01,  2.0022e-02,\n         -8.3509e-01,  4.3361e-01, -7.6710e-01, -9.8574e-01, -3.6373e-01,\n          5.2517e-01, -3.8269e-01,  5.4766e-01,  3.1082e-01, -1.0424e-01,\n         -4.9313e-01, -4.3185e-02, -2.5668e-01, -3.9637e-01, -4.0978e-01,\n         -2.3987e-01,  2.9382e-01,  1.8648e-01, -4.8027e-01, -6.4770e-02,\n          1.0662e-02,  7.9166e-01,  3.0165e-01, -7.0192e-01, -5.4201e-01,\n         -2.8544e-01,  9.4480e-01,  7.1591e-01, -2.0591e-01,  6.2725e-01,\n         -3.3718e-01, -2.8203e-01,  3.7345e-01,  4.5731e-01,  2.0396e-01,\n         -1.5658e-01, -3.8348e-01,  1.2967e-01,  2.9214e-01,  1.2057e-01,\n          8.4925e-01,  7.9554e-02,  1.0621e-01,  2.3160e-01,  2.9879e-01,\n         -2.0060e-01,  4.1204e-01, -3.6113e-01,  3.4819e-02, -1.2911e-01,\n         -2.6482e-01, -8.6733e-02, -2.6963e-01,  2.5828e-01,  2.4541e-02,\n          2.3451e-01,  7.4609e-02,  2.1660e-01,  3.4085e-01,  2.0479e-01,\n         -1.7975e-01,  4.3405e-01,  3.8206e-01,  1.8156e-01,  2.2304e-01,\n          1.3177e-01,  3.0181e-01,  4.3616e-01,  3.1888e-01, -3.9257e-01,\n          3.4389e-01,  1.1706e-01,  3.8513e-01, -2.1431e-01, -9.4444e-02,\n         -2.0363e-03,  1.0114e-02,  4.1403e-01,  1.0192e-01,  2.3394e-01,\n          1.6612e-01, -2.1375e-01,  4.2410e-01,  1.7222e-01,  3.4468e-01,\n          2.3500e-01,  4.9146e-01,  9.7406e-03, -3.4669e-01,  8.9937e-01,\n         -7.1938e-01, -2.7155e-01,  1.4888e-02, -4.7968e-01, -3.7383e-02,\n         -2.9100e-01,  3.6940e-01, -5.3822e-02, -3.1084e-01, -2.8253e-01,\n         -3.2822e-01,  4.6688e-01, -2.1135e-01, -5.2418e-02,  3.8352e-01,\n         -5.8256e-02,  5.1338e-01, -6.2399e-01,  1.1575e-01,  2.4588e-01,\n          2.4816e-01, -4.6738e-01, -8.3083e-01,  2.0673e-01, -4.3868e-01,\n         -4.5512e-01,  6.5609e-03, -5.3575e-01, -4.0410e-01, -2.5284e-01,\n         -5.0217e-01, -6.2003e-02, -1.6884e-01, -2.0019e-01,  1.2863e-01,\n          2.3084e-01,  6.8804e-01,  3.3582e-01, -4.5373e-01,  2.3566e-01,\n         -6.9388e-01, -3.3786e-01,  3.2877e-01, -4.2651e-01,  1.8241e-01,\n         -3.7247e-02, -4.4914e-01,  1.5456e-01,  2.2445e-01, -6.0973e-01,\n         -1.8574e-01, -2.8774e-01, -2.8077e-01, -2.0797e-01,  1.3094e-01,\n          4.0986e-01, -1.5432e-01, -3.9481e-02,  1.4334e-01, -5.6478e-02,\n         -4.6782e-02, -2.0909e-01,  3.4364e-01, -3.1754e-01, -2.9732e-01,\n          3.9586e-01, -1.4203e-01, -3.5289e-03,  1.0159e+00,  5.3215e-02,\n         -2.6630e-01, -6.8021e-01, -3.6894e-01,  1.7932e-02,  2.8158e-01,\n         -3.0466e-02,  2.4336e-01,  4.8912e-01,  2.6125e-01,  8.3158e-01,\n          2.4576e-02,  6.5444e-01,  3.4801e-01, -2.2360e-01,  3.4719e-01,\n         -3.2160e-01,  1.4535e-01,  1.3449e-01, -2.1402e-01,  3.8996e-01,\n         -4.5644e-02, -8.7515e-02,  4.8348e-02, -2.3754e-01,  9.7126e-02,\n         -3.2940e-01,  1.0199e+00,  5.9858e-01,  1.7349e-01,  3.7291e-01,\n         -1.2701e-01,  7.6884e-02, -2.9925e-02,  4.7105e-01, -4.0411e-02,\n         -1.0054e-01, -3.7895e-01,  1.9462e-03,  2.2897e-01,  5.6860e-02,\n          6.3284e-01,  6.0423e-01,  2.0631e-01,  7.8487e-02,  1.2332e+00,\n         -3.7361e-01, -2.3601e-01,  2.0394e-01, -5.3406e-01, -6.8335e-02,\n         -6.6997e-01,  5.0838e-01, -1.1846e-01,  7.5253e-02,  2.8233e-01,\n         -4.8246e-02, -2.6180e-01,  2.9794e-02,  4.6994e-01, -5.8860e-02,\n         -7.0060e-01],\n        [ 2.1355e-01, -1.7607e-01, -3.5476e-01, -5.1527e-01,  8.1717e-02,\n          2.2581e-01, -4.0596e-01, -7.3162e-02, -1.0103e-01,  2.8420e-02,\n         -6.2000e-02, -3.8737e-01,  2.3103e-01, -1.4420e-01,  1.0285e-01,\n          2.8502e-02,  6.5597e-02, -2.0660e-01,  3.2033e-01,  8.5394e-02,\n          1.9705e-01,  2.3309e-02,  3.1297e-01,  7.3046e-02, -3.7009e-01,\n          1.5268e-01, -1.1129e-02,  2.8483e-02,  2.8089e-02, -4.2249e-04,\n         -4.8536e-02, -3.6178e-01, -5.6584e-01,  9.7172e-02,  2.7697e-01,\n         -4.4250e-01, -1.4757e-02, -4.1948e-01, -6.3146e-01,  4.4217e-02,\n          5.6749e-01, -2.9137e-01,  4.8410e-01,  7.3446e-01,  1.3406e-01,\n         -7.6137e-01, -6.9541e-02, -5.2326e-01, -2.9035e-01, -2.8280e-01,\n         -1.5908e-02, -2.8162e-02, -4.0686e-02, -2.4128e-01, -8.4039e-02,\n          1.3853e-01,  2.5519e-01,  3.8158e-01, -4.7837e-01, -3.9920e-01,\n         -3.4941e-01,  4.4816e-01,  2.6296e-01, -2.1550e-01,  3.0128e-01,\n          8.4365e-03, -9.7907e-03,  2.5629e-01,  5.3351e-01, -1.5934e-01,\n         -3.8094e-02, -2.2578e-01,  4.1746e-01,  1.1801e-01,  1.8301e-01,\n          7.7730e-01,  1.5896e-01, -4.2364e-01, -3.6109e-02,  4.6272e-01,\n          6.4736e-03,  1.7275e-01, -1.6943e-01, -8.4692e-03,  1.3917e-01,\n         -3.6347e-01, -1.3758e-01, -2.8171e-01,  4.5611e-01, -4.0884e-01,\n          2.9959e-01, -1.2114e-01,  1.4736e-01,  3.4997e-01,  2.6477e-01,\n          2.5636e-01,  1.1899e-01,  1.0772e-01, -2.2602e-02,  2.7426e-01,\n         -2.5548e-01,  9.8435e-02,  2.8691e-01,  1.4841e-01, -3.4290e-01,\n          2.7301e-01,  1.3680e-01,  2.9884e-01, -1.2250e-01,  3.5721e-02,\n          2.2716e-01,  2.1858e-01,  2.0649e-01,  2.7237e-01,  1.3027e-01,\n          3.6778e-01, -5.5441e-01,  3.7907e-01, -1.0926e-02,  3.5090e-01,\n          3.4240e-02,  3.2378e-01, -1.2610e-01, -8.3539e-02,  5.4741e-01,\n         -6.4451e-01,  2.4141e-02,  1.3956e-01, -2.9531e-01, -2.1117e-01,\n         -3.1962e-01,  1.1964e-01,  1.3166e-01, -4.2019e-01, -1.7331e-01,\n         -3.8925e-01,  1.2351e-01, -1.7745e-01,  1.1576e-01, -4.6240e-02,\n         -8.7951e-02,  3.9368e-01, -3.3016e-01,  2.6049e-01, -2.4988e-01,\n          3.7355e-02, -3.3731e-01, -6.0356e-01, -8.6488e-02,  1.4144e-02,\n         -3.4583e-01,  1.5316e-03, -3.4877e-01, -1.0780e-01,  4.2908e-02,\n         -4.1509e-01, -2.9651e-02, -1.8015e-01, -6.9207e-02, -2.1601e-02,\n          2.9621e-01,  8.1626e-01,  1.7956e-02, -3.0876e-01,  2.3153e-01,\n         -4.9407e-01,  4.3472e-02,  6.7787e-02, -2.4018e-01,  1.2864e-01,\n         -4.1391e-04, -3.6014e-01, -1.0105e-01,  5.5262e-01, -1.9475e-01,\n         -6.3003e-02, -3.8291e-01,  6.8258e-02, -1.4552e-01, -1.6010e-01,\n          3.3342e-01, -2.2938e-01,  9.9288e-02,  2.8094e-01,  3.6334e-01,\n          2.5014e-01, -2.8317e-01,  3.0510e-02,  1.9568e-01, -2.5957e-01,\n          4.0025e-01,  8.0130e-02,  3.0041e-02,  7.3000e-01, -1.3715e-01,\n         -5.5724e-01, -3.3584e-01, -5.3724e-03,  1.1283e-01,  4.1086e-01,\n         -2.0004e-01,  2.4406e-01,  1.4016e-01,  1.8895e-01,  6.7835e-01,\n          1.2844e-01,  4.1716e-01,  4.0917e-01, -3.3431e-01,  1.6755e-01,\n         -2.3698e-01,  1.2342e-01,  3.0405e-01, -8.7513e-02, -1.0241e-01,\n          6.9318e-03, -1.0632e-01,  2.2356e-01, -1.0422e-01, -2.9323e-01,\n          2.9477e-01,  8.9637e-01,  5.1328e-01,  2.2793e-02,  8.2223e-02,\n         -3.6547e-01,  1.1150e-01, -3.7625e-01, -2.1489e-01, -4.1515e-01,\n         -2.7153e-01, -4.9397e-01, -2.4366e-01,  2.4539e-01,  3.8880e-02,\n          2.7381e-01,  2.4084e-01, -1.5367e-01, -1.3360e-01,  8.6506e-01,\n          8.5748e-02, -5.4889e-01,  3.8994e-01, -4.3183e-01, -3.1406e-01,\n         -4.8208e-01,  1.2394e-01,  1.1177e-01, -1.6237e-01, -2.2637e-01,\n          7.9493e-02, -1.1618e-01, -8.0067e-02,  1.3471e-01, -4.3758e-02,\n         -3.2496e-01]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            self._modules[block] = block\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "            return X\n",
    "net = MySequential(nn.Linear(20, 256),nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T14:11:26.589930Z",
     "end_time": "2023-07-10T14:11:26.614863Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在正向传播函数中执行代码"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.1612, grad_fn=<SumBackward0>)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        # weight不参加训练\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T14:17:17.587187Z",
     "end_time": "2023-07-10T14:17:17.599155Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "混合搭配各种组合块的方法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.0682, grad_fn=<SumBackward0>)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T14:19:50.341866Z",
     "end_time": "2023-07-10T14:19:50.349844Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
